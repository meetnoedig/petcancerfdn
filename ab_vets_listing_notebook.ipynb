{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Extract data from an HTML table using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped and saved the directory data to /home/ubuntu/PCF/CSVs/abvma.csv.\n"
     ]
    }
   ],
   "source": [
    "# Make a request\n",
    "url = 'https://abvma.ca/company/roster/companyRosterView.html?companyRosterId=23'\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code != 200:\n",
    "    print(f'Request failed with status code {response.status_code}')\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Create a CSV file and write the headers\n",
    "filename = '/home/ubuntu/PCF/CSVs/abvma.csv'\n",
    "with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Practice Name', 'Responsible Vet', 'Species', 'City', 'Province', 'Postal', 'Phone', 'Fax', 'Accredited', 'Physical location'])\n",
    "\n",
    "    # Loop through each directory entry and extract the data\n",
    "    directory_entries = soup.find_all('div', class_='directory-entry')\n",
    "    for entry in directory_entries:\n",
    "        name = entry.find('h4', class_='company-name').text.strip()\n",
    "        responsible_vet = entry.find('p', class_='responsible-vet').text.strip()\n",
    "        species = entry.find('p', class_='species').text.strip()\n",
    "        city = entry.find('p', class_='city').text.strip()\n",
    "        province = entry.find('p', class_='province').text.strip()\n",
    "        postal_code = entry.find('p', class_='postal-code').text.strip()\n",
    "        phone = entry.find('p', class_='phone').text.strip()\n",
    "        fax = entry.find('p', class_='fax').text.strip()\n",
    "        accredited = entry.find('p', class_='accredited').text.strip()\n",
    "        physical_location = entry.find('p', class_='physical-location').text.strip()\n",
    "        writer.writerow([name, responsible_vet, species, city, province, postal_code, phone, fax, accredited, physical_location])\n",
    "\n",
    "print(f'Successfully scraped and saved the directory data to {filename}.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Extract data from an HTML table using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     19\u001b[0m practice_name \u001b[39m=\u001b[39m cols[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_text()\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m---> 20\u001b[0m responsible_vet \u001b[39m=\u001b[39m cols[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39mget_text()\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     21\u001b[0m species \u001b[39m=\u001b[39m cols[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mget_text()\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     22\u001b[0m city \u001b[39m=\u001b[39m cols[\u001b[39m3\u001b[39m]\u001b[39m.\u001b[39mget_text()\u001b[39m.\u001b[39mstrip()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "url = 'https://abvma.ca/company/roster/companyRosterView.html?companyRosterId=23'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the data\n",
    "table = soup.find('table')\n",
    "\n",
    "# Open a CSV file for writing\n",
    "with open('/home/ubuntu/PCF/CSVs/abvma.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Practice Name', 'Responsible Vet', 'Species', 'City', 'Province', 'Postal', 'Phone', 'Fax', 'Accredited', 'Physical location'])\n",
    "\n",
    "    # Loop through each row in the table and extract the data\n",
    "    for row in table.find_all('tr'):\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) == 0:\n",
    "            continue\n",
    "        practice_name = cols[0].get_text().strip()\n",
    "        responsible_vet = cols[1].get_text().strip()\n",
    "        species = cols[2].get_text().strip()\n",
    "        city = cols[3].get_text().strip()\n",
    "        province = cols[4].get_text().strip()\n",
    "        postal = cols[5].get_text().strip()\n",
    "        phone = cols[6].get_text().strip()\n",
    "        fax = cols[7].get_text().strip()\n",
    "        accredited = cols[8].get_text().strip()\n",
    "        physical_location = cols[9].get_text().strip()\n",
    "\n",
    "        writer.writerow([practice_name, responsible_vet, species, city, province, postal, phone, fax, accredited, physical_location])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3: Extract data from an HTML table using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m writer\u001b[39m.\u001b[39mwriterow([\u001b[39m'\u001b[39m\u001b[39mPractice Name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mResponsible Vet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSpecies\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mProvince\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPostal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPhone\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFax\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAccredited\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPhysical location\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m \u001b[39m# Loop through each row in the table and extract the data\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m table\u001b[39m.\u001b[39;49mfind_all(\u001b[39m'\u001b[39m\u001b[39mtr\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     18\u001b[0m     cols \u001b[39m=\u001b[39m row\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mtd\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(cols) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "# Send a request to the website\n",
    "url = \"https://abvma.ca/company/roster/companyRosterView.html?companyRosterId=23\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table that contains the data we want\n",
    "table = soup.find('table', {'class': 'table table-striped'})\n",
    "\n",
    "# Create a CSV file to store the data\n",
    "with open('/home/ubuntu/PCF/CSVs/abvma.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Practice Name', 'Responsible Vet', 'Species', 'City', 'Province', 'Postal', 'Phone', 'Fax', 'Accredited', 'Physical location'])\n",
    "\n",
    "    # Loop through each row in the table and extract the data\n",
    "    for row in table.find_all('tr'):\n",
    "        cols = row.find_all('td')\n",
    "\n",
    "        if len(cols) == 0:\n",
    "            continue\n",
    "\n",
    "        practice_name = cols[0].text.strip()\n",
    "        responsible_vet = cols[1].text.strip()\n",
    "        species = cols[2].text.strip()\n",
    "        city = cols[3].text.strip()\n",
    "        province = cols[4].text.strip()\n",
    "        postal = cols[5].text.strip()\n",
    "        phone = cols[6].text.strip()\n",
    "        fax = cols[7].text.strip()\n",
    "        accredited = cols[8].text.strip()\n",
    "        physical_location = cols[9].text.strip()\n",
    "\n",
    "        # Write the data to the CSV file\n",
    "        writer.writerow([practice_name, responsible_vet, species, city, province, postal, phone, fax, accredited, physical_location])\n",
    "\n",
    "print(\"Data has been scraped and saved to /home/ubuntu/PCF/CSVs/abvma.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to scrape data and save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the data from the website\n",
    "def get_data(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "#Function to parse the data\n",
    "def parse_data(soup):\n",
    "    table = soup.find('table', attrs={'class': 'table table-striped'})\n",
    "    if table is not None:\n",
    "        rows = table.find_all('tr')\n",
    "        data = []\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            data.append([ele for ele in cols if ele]) # Get rid of empty values\n",
    "        return data\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "#write to csv with column names: name, vet_clinic_name, vet_clinic_type\n",
    "def write_csv(data):\n",
    "    with open('abvma.csv', 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['name', 'vet_clinic_name', 'vet_clinic_type'])\n",
    "        for name in data:\n",
    "            writer.writerow(name)\n",
    "#main\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://www.abvma.ca/client/roster/clientRosterView.html?clientRosterId=23'\n",
    "    soup = get_data(url)\n",
    "    data = parse_data(soup)\n",
    "    write_csv(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ookla-statcan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91ca161aeacff7984e655859ba268e97138d6bd1e60cc428aeb0e3f4bf3cb092"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
